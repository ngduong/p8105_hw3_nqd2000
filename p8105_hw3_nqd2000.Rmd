---
title: "P8105_HW3_nqd2000"
author: "Ngoc Duong"
date: "10/10/2019"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r, message = FALSE, warning = FALSE}
library(readr)
library(tidyverse)
library(viridis)
library(ggplot2)
library(leaflet)
library(dplyr)
library(p8105.datasets)
```

## Problem 1 

```{r instacart dataset}
#get instacart dataset 
data(instacart)
```
This `instacart` dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. Variables that might be helpful include:
<br /> `aisle`: aisle name (e.g, `r unique(sample(as.factor(pull(instacart,aisle)), 4, replace=FALSE))`, etc.),
<br /> `department`: department name (e.g, `r unique(sample(as.factor(pull(instacart,department)), 4, replace=FALSE))`, etc.),
<br /> `product_name`: name of product ordered (e.g, `r unique(sample(as.factor(pull(instacart,product_name)), 4, replace=FALSE))`, etc.),
<br /> `order_id`: order identifier (observations with the same `order_id` means products ordered are in a same basket, identified by other variables as listed next),
<br /> `order_dow`: day of week when product was ordered (e.g, take "0" as Sunday, "1" as Monday, "2" as Tuesday, and so on),
<br /> `order_hour_of_day`: hour of day when product was ordered (e.g, "1" means 1am, "14" means 2pm, "23" means 11pm, and so on),
<br /> `reordered`: whether the product has been ordered before ("1" means the product has been ordered before and "0" means otherwise),
<br /> and `days_from_prior_order`: number of days between newest order (reorder) and previous order (ranging from 0 days to 30 days).

All these variables are interconnected. Variables with containing `_id` might be more helpful for internal use (efficient information storing), as they correspond with the name ("Bulgarian Yogurt" has `product_id` of 49302, or aisle "Fresh Fruits" has `aisle_id` 24, etc.)

It was noted that the original data is more extensive (having about 3 million observations), and the dataset here is a cleaned and limited version of the original one. Since the variable `eval_set` only contains value `train`, it might be the case that this dataset was subsetted from the original one as a training dataset for some model training purpose.

```{r top aisles}
#calculate the number of orders for each aisle 
top_aisles <- instacart %>% 
                mutate(                              #convert to factor some variables
                  aisle = factor(aisle),
                  department = factor(department)
                ) %>% 
                group_by(aisle) %>%                  #organize dataset by aisle
                summarize(order_count = n()) %>%     #count number of "orders"/frequency within each aisle "group"                            
                mutate(                              #order aisle based on calculated order count for each
                  aisle = fct_reorder(aisle, order_count)
                )
#I used `sum(is.na(instacart))` to check if there was any NA values. Since there was no NA's, I decided not to use the `drop_na()` function.

top_aisles %>% 
  top_n(5) %>%                                       #get top 5 aisles (based on order counts)
  knitr::kable()                                     #create a table 
```

The cleaned dataset (`top_aisles`) on aisles and their order counts is a `r nrow(top_aisles)` x `r ncol(top_aisles)` tibble showing `r nrow(top_aisles)` different aisles and the corresponding number of orders for each aisle. 
From the list above, top 5 aisles with most items ordered are `r top_aisles %>% top_n(5) %>% .[1] %>% pull()`. Among the top 3 aisles with highest order frequencies -- `r top_aisles %>% top_n(3) %>% .[1] %>% pull()`, the order counts are `r top_aisles %>% top_n(3) %>% .[2] %>% pull()`, respectively. 

Now we can use some visualization to look at the aisles and their order counts:

```{r plot items against aisle}
# Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

top_aisles %>% 
    filter(order_count > 10000) %>%      #limit to only aisles with more than 10000 items ordered 
    ggplot(aes(x = aisle,                #start plotting    
               y = order_count,
               fill = order_count)) +
    geom_bar(stat = "identity") +
    viridis::scale_fill_viridis(
      begin = 1, end = 0) +              #specify color range so highest order counts get darker color
    coord_flip() +                       #flip coordinates so aisle names are readable
    labs(                                #add title, x-axis and y-axis titles
      title = "Number of items ordered in each aisle",
      x = "Aisle name",
      y = "Number of items") + 
    theme_bw() +                         #set theme black and white, place legend below plot, and format legend
    theme(legend.position = "bottom",    
          plot.title = element_text(hjust = 0.5, size=12, face='bold')) 
```

The top two aisles with most producted ordered from have approximately twice the product counts as the third most popular aisle (packaged vegetables and fruits). After that product order counts tapered off dramatically as popularity rank decreases.

```{r three most popular items}
## Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.  Include the number of times each item is ordered in your table.

instacart %>% 
 filter(aisle %in% 
          c("baking ingredients", 
            "dog food care",
            "packaged vegetables fruits")) %>%     #filter out these specific aisles 
    group_by(aisle, product_name) %>%              #organize by product name and aisle to count
    summarize(orders = n()) %>%                    #create variable orders as frequency count of product_name within each aisle
  top_n(n = 3) %>%                                 #select top 3 popular product within each aisle
  arrange(desc(orders), .by_group = TRUE) %>%      #arrange product by descending order of counts
  knitr::kable()                                   #create a table
```

From this table, we can see among aisle `packaged vegetables and fruits`, `organic baby spinach` has the most (and significantly higher) order counts than the second and third product `organic raspeberries`, and `organic blueberries`. 

```{r mean hour of day for PLA and CIC}

instacart %>% 
 filter(product_name %in%                        #select specific products of interest
          c("Pink Lady Apples", 
            "Coffee Ice Cream")) %>% 
 group_by(order_dow, product_name) %>%           #organize by day of order and product
 arrange(order_dow) %>%                          #arrange day of order
 summarize(mean_hour =                           #compute average hour of order for each group
             mean(order_hour_of_day)) %>% 
 ungroup(order_dow) %>%                          #ungroup to mutate
 mutate(order_dow =                              #recode values in day of order
    recode(order_dow, 
       "0"="Sunday",
       "1"="Monday",
       "2"="Tuesday",
       "3"="Wednesday",
       "4"="Thursday",
       "5"="Friday",
       "6"="Saturday")) %>% 
 pivot_wider(names_from = "order_dow",           #create a reader-friendly wide dataframe
             values_from = "mean_hour") %>% 
 knitr::kable(digits = 2)                        #create table and specify decimal numbers
```

From the table, we can see that coffee ice cream was usually ordered at, on average, 2-3pm on weekdays and 12-2pm on weekends, whereas the average hour (of day) for ordering Pink Lady Apples was more around noon and consistent. Since 2-3pm might be the time most people feel tired from working, they might have the tendency to treat themselves (with caffeinated products) and thus coffee ice cream. 

## Problem 2
```{r load brfss2010 data}
#get brfss_2010 dataset
data("brfss_smart2010")

#some data cleaning
brfss_df <- brfss_smart2010 %>% 
            janitor::clean_names() %>%      #clean names
  rename(                                   #simplify names 
    state = locationabbr,              
    county = locationdesc) %>% 
  separate(                                 #separate county into actual county name and state name abbreviation
    county, 
    into = c("state_abb", "county"), 
    sep = " - ") %>% 
  select(-state_abb) %>%                    #get rid of state name abbreviations
  mutate(                                   #change data type of some variables
    state = factor(state),                  #make state, county, topic, and response as factor variables
    county = factor(county),
    topic = factor(str_to_lower(topic)),    #recode values to lower cases while making factor
    response = factor(str_to_lower(response))) %>% 
  filter(                                   #filter out to focus on only "overall health" topic
    topic == "overall health",
    response %in% c("poor", "fair", "good", "very good", "excellent")) %>% #
  mutate(                                   #order values from "poor" to "excellent"
    response = factor(
                  response, 
                  ordered = TRUE, 
                  levels = c("poor", 
                             "fair", 
                             "good", 
                             "very good", 
                             "excellent")))
```

```{r}                
#find location number observed in 2002 and 2010
#in 2002
brfss_df %>%
  filter(year == 2002) %>%                      #filter only year 2002
  select(year, state, county) %>%               #select variables of interest
  group_by(year, state) %>%                     #group by year, state so each combination is treated as a consistent chunk
  distinct(county) %>%                          #get rid of duplicate counties in each state
  mutate(obs_locations = n()) %>%               #create obs_locations as number of counties (locations) within each state
  filter(obs_locations > 6) %>%                 #filter number of counties >6
  distinct(year, state, obs_locations) %>%      #get rid of duplicates for final list
  pivot_wider(                                  #make reader-friendly wide-form dataframe
    names_from = state,
    values_from = obs_locations) %>% 
  knitr::kable()                                #make table 

#in 2010
brfss_df %>%
  filter(year == 2010) %>%                      #filter only year 2002
  select(year, state, county) %>%               #select variables of interest
  group_by(year, state) %>%                     #group by year, state
  distinct(county) %>%                          #collapse duplicate counties in each state
  mutate(
    obs_locations = n()) %>%                    #create obs_locations as number of counties (locations) within each state
  filter(
    obs_locations > 6) %>%                      #filter number of counties >6
  distinct(year, state, obs_locations) %>%      #get rid of duplicates for final list
  pivot_wider(                                  #make reader-friendly wide-form dataframe
    names_from = state,
    values_from = obs_locations) %>% 
  knitr::kable()                                #make table 
```

From the tables above, we could see in 2002, states that were observed at 7 or more locations were CT, FL, MA, NJ, NC, and PA; in 2010, states that were observed at 7 or more locations were CA, CO, FL, MD, MA, NE, NJ, NY, NC, OH, PA, SC, TX, and WA.

CT fell out of the list of states with 7 or more observation locations, while FL AND NJ increased their number of locations dramatically (7 to 41, and 8 to 19, respectively). MA and NC had less dramatic increases in the number of observation locations, and PA decreased from 10 locations to 7. Bigger states seem to have more observation locations (CA with 12, TX with 16, and WA with 10), although MD also have relatively more obversation locations than average (12).

```{r}
#dataset with excellent responses, with year, state, and a variable that averages the data_value across locations within a state. 

excellent_response_df <- brfss_df %>%
  filter(
    response == "excellent") %>%                 #filter out only "excellent" responses
  select(year, state, data_value) %>%            #select variables of interest
  group_by(year, state) %>%                      #group/organize to compute measurements about state
  summarize(                                     #compute average data_value across locations within state
    mean_value = mean(data_value, na.rm = TRUE))
```

```{r}
#“spaghetti” plot of this average value within each state across years 
excellent_response_df %>% 
 ggplot(                                         #start plot
  aes(x = year,                                  #specify values that go to each axis and line color (by state)
      y = mean_value,  
      colour = state)) +
  geom_line() +                                  #specify using line for spaghetti plot
  viridis::scale_color_viridis(                  #use viridis color palette
    begin = 0, 
    discrete = TRUE) +
  labs(                                          #add titles
    title = "Distribution of average excellent responses (in percentage)",
    subtitle = "for every state from 2002 to 2010",
    x = "Year",
    y = "Excellent responses (%)") +
  theme(legend.position = "right",               #format legend and title
        legend.title = 
          element_blank(),
        plot.title = 
          element_text(hjust = 0.5, size=12, face='bold'),
        plot.subtitle = 
          element_text(hjust = 0.5)
        )
```

From the graph, we can see that the state represented by yellow-hued line was behind the curve in terms of proportion of responses that are excellent, specifically West Virginia. Some states that are represented by the purple-hued lines seem to be ahead of the curve, including CO, CT, and DC.

```{r}
#make two-panel graph for distribution inside 
brfss_df %>%
  filter(year %in% c("2006", "2010"),            #get only year 2006 and 2010 and NY state
         state == "NY") %>% 
  ggplot(aes(                                    #specify values that go to each axis and line color (by county)
    x = response, 
    y = data_value, 
    fill = county)) +
  geom_bar(                                      #specify graph type -- using barplot 
    stat = "identity", 
    position=position_dodge()) +                 #specify unstacked bars
  labs(                                          #add title
    title = "Distribution of mean overall health survey responses in NY in 2006 and 2010",
    x = "Average overall health survey responses", 
    y= "",
    caption = "Data from BRFSS") +
  theme(legend.position = "bottom",              #format legend 
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5, size=12, face='bold')) +
  viridis::scale_fill_viridis(                   #use viridis color palette
       begin = 0,
       discrete = TRUE) + 
  facet_grid(year ~ .)                           #create two panels split by year
```

The plots show that there were high frequency of "Good", "Very good", and "Excellent" responses in 2006 and 2010, with a little more responses falling into the former two categories. No dramatic change was observed comparing 2006 to 2010, although there were the new locations in 2010 (Bronx, Erie, and Kings). 

## Problem 3
```{r}
#load dataset
accel_df <- read_csv("./accel_data.csv")

#cleaning
accel_clean <- accel_df %>%  
  janitor::clean_names() %>%                        #clean names
  pivot_longer(activity_1:activity_1440,            #transform dataset from wide to long
               names_to = "minute",                 
               values_to = "activity_level") %>% 
  separate(                                         #extract only the minute (number) part of minute variable
    minute,  
    into = c("activity", "minute"), 
    sep = "_") %>% 
  select(-activity) %>%                             #deselect redundant variable (from separate)
  mutate(                                           #change data type of some variables
    week = factor(week),
    day_id = factor(day_id),
    day_of_week =                                   #create a weekday vs. weekend variable
      factor(case_when(day %in% c("Saturday","Sunday") ~ "weekend", 
               (day != "Saturday" | day != "Sunday") ~ "weekday")),
    minute = as.numeric(minute),
    activity_level = as.numeric(activity_level),
    day = factor(day,                               #order factor day with appropriate ordinal levels 
                 ordered = TRUE, 
                 levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))
```

The resulting dataset `accel_clean` after cleaning is of long-format, having `r nrow(accel_clean)` observations (covering 1440 minutes in each day and spanning 35 days) and `r ncol(accel_clean)` variables. The variables are:
<br/> `week`: indicating which week the observation was collected in (ranging 1-5),
<br/> `day_id`: indicating the number of day (values range from 1-35),
<br/>: `day`: indicating which day of the week, corresponding to `day_id` (e.g, day_id = 1 is Friday in week 1, then day_id = 2 is Saturday, and day_id = 8 is Friday in week 2)
<br/> `activity_level`: how much activity was recorded by the tracker for each minute
<br/> `day_of_week`: specifying which type of day a certain day is (e.g, weekend if `day` is "Saturday" or "Sunday", and weekday otherwise).
The mean activity level (per minute) of this man over the period of 5 weeks (35 days) is `r accel_clean %>% pull(activity_level) %>% mean() %>% round(2)`. 
The average activity level (per minute) on a weekday is `r filter(accel_clean, day_of_week == "weekday") %>% pull(activity_level) %>% mean() %>% round(2)`, and the median is `r filter(accel_clean, day_of_week == "weekday") %>% pull(activity_level) %>% median() %>% round(2)`, compared to the average activity level (per minute) on a weekend of `r filter(accel_clean, day_of_week == "weekend") %>% pull(activity_level) %>% mean() %>% round(2)`, and median of `r filter(accel_clean, day_of_week == "weekend") %>% pull(activity_level) %>% median() %>% round(2)`.

More specific: the average activity level (per minute) on a Wednesday is `r filter(accel_clean, day == "Wednesday") %>% pull(activity_level) %>% mean() %>% round(2)`, compared to the average activity level (per minute) on a Sunday of `r filter(accel_clean, day == "Sunday") %>% pull(activity_level) %>% mean() %>% round(2)`.


```{r table of daily activity count}
#make table of aggregate daily activity count across days of week for 5 weeks
accel_clean %>%  
  group_by(week, day_id, day) %>%                  #group by variables of interest
  mutate(                                          #sum up activity level within each group (not counting NA's)
    total_activity = 
      round(
        sum(activity_level, na.rm = TRUE),
        1)) %>% 
  ungroup() %>%                                    #ungroup so factor variables (such as week) can collapse when making table
  select(c("week", "day", "total_activity")) %>%   #select variables of interest for table
  distinct() %>%                                   #get rid of duplicate rows for table
  pivot_wider(                                     #make reader-friendly wide dataframe
    names_from = "day", 
    values_from = "total_activity") %>%            #arrange column in table 
    select(c("week", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")) %>%        
knitr::kable()                                     #create table
```

The table suggests that, on average, there seem to be no large deviations in the levels of daily activity across days and weeks, except for a couple of days. More specifically, this man's activity level seems to be consistent across most weekdays and weekends for the first three weeks. On the fourth and fifth weeks, his activity level decreased during the weekend (compared to weekdays and previous weekends). Outliers on a lower activity scale can be observed on week 1's Monday, week 4's Friday, and the 4th and 5th weekend. The extreme low numbers on Monday of week 1, and Saturday of week 4 and week 5 might stem from that this man did not wear the tracker/only wore it for part of the day/stayed at home and was idle/sick, or the tracker was out of battery.

```{r daily activitiy scatterplot}
#plot daily activity across 24 hours for 5 weeks
accel_clean %>%  
  group_by(week, day) %>%                   #group by week and day
  ggplot(aes(
    x = minute/60,                          #specify variables that are presented by axes and coloring (divide minutes by 60 to obtain corresponding "hour of day")
    y = activity_level, 
    fill = day)) +                          #color of datapoints indicate day of week
  geom_point(                               #specify coloring for data points and format
    aes(color = day), 
    size = 0.7,
    alpha = 0.7) +
  labs(                                     #add titles
    title = "Accelerometer data on 24-hour days across days of the week",
    subtitle = "collected on a 63 year-old male with BMI 25",
    x = "Hour",
    y = "Activity level") +
  theme(                                    #format legend and plot title 
    legend.position = "bottom",         
    plot.title =        
          element_text(hjust = 0.5, size=12, face='bold'),
    plot.subtitle = 
          element_text(hjust = 0.5),
    legend.title =
          element_blank()) +
  scale_x_continuous(                        #choose interval width for x- and y-axis
    breaks = seq(0, 24, 
                 by = 1)) + 
  scale_y_continuous(
    breaks = seq(0, 9000, 
                 by = 1500)) +
  viridis::scale_color_viridis(              #choose viridis color palette
    discrete = TRUE)
```

The plot suggests expected/normal activity level for a person depending on time of day and day of week. Low activity levels were recorded at night -- after 11pm until 6am. In the morning, there seems to be surges in activity levels during 6am-8am on weekdays (potentially waking up, getting ready, and going to work/other activity), and on weekends, the same surges can be observed a little later (9am-11am). High levels of activity can also be observed at nighttime (7pm-10pm) on the weekdays. This is probably when he goes out, or do other activities at home (outside of work).

